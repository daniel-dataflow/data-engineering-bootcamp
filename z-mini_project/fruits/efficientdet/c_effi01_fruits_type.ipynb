{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d9e764",
   "metadata": {},
   "source": [
    "# EfficientDet - Í≥ºÏùº Ï¢ÖÎ•ò Î∂ÑÎ•ò (ÏÇ¨Í≥º/Î∞∞/Í∞ê)\n",
    "## Î™©Ìëú: cate1 Í∏∞Î∞ò 3-class Object Detection\n",
    "\n",
    "**Classes:**\n",
    "- 0: ÏÇ¨Í≥º\n",
    "- 1: Î∞∞\n",
    "- 2: Í∞ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae6b8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n",
      "Base Directory: /Users/handaeseong/dev/data-engineer/workspaces/z-mini_project/fruits\n",
      "Dataset Directory: /Users/handaeseong/dev/data-engineer/workspaces/z-mini_project/fruits/datasets\n",
      "Images Directory: /Users/handaeseong/dev/data-engineer/workspaces/z-mini_project/fruits/datasets/images\n",
      "Labels Directory: /Users/handaeseong/dev/data-engineer/workspaces/z-mini_project/fruits/datasets/json_labels\n"
     ]
    }
   ],
   "source": [
    "# 1. ÌôòÍ≤Ω ÏÑ§Ï†ï Î∞è ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "BASE_DIR = os.path.abspath('..')\n",
    "DATASET_DIR = os.path.join(BASE_DIR, 'datasets')\n",
    "IMAGES_DIR = os.path.join(DATASET_DIR, 'images')\n",
    "LABELS_DIR = os.path.join(DATASET_DIR, 'json_labels')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'efficientdet', 'outputs')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Base Directory: {BASE_DIR}\")\n",
    "print(f\"Dataset Directory: {DATASET_DIR}\")\n",
    "print(f\"Images Directory: {IMAGES_DIR}\")\n",
    "print(f\"Labels Directory: {LABELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16dba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ï¥ù JSON ÌååÏùº Ïàò: 70\n",
      "\n",
      "ÌååÏùº: apple_fuji_M_26-57.json\n",
      "  cate1: ÏÇ¨Í≥º\n",
      "  cate3: ÏÉÅ\n",
      "  bndbox: {'xmin': 0, 'ymin': 0, 'xmax': 1000, 'ymax': 1000}\n",
      "\n",
      "ÌååÏùº: persimmon_booyu_S_76-26_1TOP.json\n",
      "  cate1: Í∞ê\n",
      "  cate3: Î≥¥ÌÜµ\n",
      "  bndbox: {'xmin': 0, 'ymin': 0, 'xmax': 1000, 'ymax': 1000}\n",
      "\n",
      "ÌååÏùº: persimmon_booyu_L_76-30_5DI90.json\n",
      "  cate1: Í∞ê\n",
      "  cate3: Ìäπ\n",
      "  bndbox: {'xmin': 0, 'ymin': 0, 'xmax': 1000, 'ymax': 1000}\n",
      "\n",
      "ÌååÏùº: persimmon_booyu_M_76-33_3FR90.json\n",
      "  cate1: Í∞ê\n",
      "  cate3: ÏÉÅ\n",
      "  bndbox: {'xmin': 0, 'ymin': 0, 'xmax': 1000, 'ymax': 1000}\n",
      "\n",
      "ÌååÏùº: apple_fuji_S_26-57.json\n",
      "  cate1: ÏÇ¨Í≥º\n",
      "  cate3: Î≥¥ÌÜµ\n",
      "  bndbox: {'xmin': 0, 'ymin': 0, 'xmax': 1000, 'ymax': 1000}\n",
      "\n",
      "=== Îç∞Ïù¥ÌÑ∞ÏÖã ÌÜµÍ≥Ñ ===\n",
      "Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò: 70\n",
      "Ï¥ù Í∞ùÏ≤¥ Ïàò: 70\n",
      "  ÏÇ¨Í≥º: 30Í∞ú (42.9%)\n",
      "  Î∞∞: 10Í∞ú (14.3%)\n",
      "  Í∞ê: 30Í∞ú (42.9%)\n"
     ]
    }
   ],
   "source": [
    "# 2. Îç∞Ïù¥ÌÑ∞ÏÖã Î∂ÑÏÑù Î∞è ÌÜµÍ≥Ñ\n",
    "def analyze_dataset():\n",
    "    \"\"\"Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï°∞ Î∂ÑÏÑù\"\"\"\n",
    "    json_files = glob.glob(os.path.join(LABELS_DIR, '*.json'))\n",
    "    \n",
    "    class_counts = {'ÏÇ¨Í≥º': 0, 'Î∞∞': 0, 'Í∞ê': 0}\n",
    "    total_boxes = 0\n",
    "    \n",
    "    print(f\"\\nÏ¥ù JSON ÌååÏùº Ïàò: {len(json_files)}\")\n",
    "    \n",
    "    for json_file in json_files[:5]:  # ÏÉòÌîå 5Í∞ú Ï∂úÎ†•\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            print(f\"\\nÌååÏùº: {os.path.basename(json_file)}\")\n",
    "            print(f\"  cate1: {data.get('cate1', 'N/A')}\")\n",
    "            print(f\"  cate3: {data.get('cate3', 'N/A')}\")\n",
    "            print(f\"  bndbox: {data.get('bndbox', {})}\")\n",
    "    \n",
    "    # Ï†ÑÏ≤¥ ÌÜµÍ≥Ñ\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            cate1 = data.get('cate1', '')\n",
    "            if cate1 in class_counts:\n",
    "                class_counts[cate1] += 1\n",
    "                total_boxes += 1\n",
    "    \n",
    "    print(f\"\\n=== Îç∞Ïù¥ÌÑ∞ÏÖã ÌÜµÍ≥Ñ ===\")\n",
    "    print(f\"Ï¥ù Ïù¥ÎØ∏ÏßÄ Ïàò: {len(json_files)}\")\n",
    "    print(f\"Ï¥ù Í∞ùÏ≤¥ Ïàò: {total_boxes}\")\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"  {cls}: {count}Í∞ú ({count/total_boxes*100:.1f}%)\")\n",
    "    \n",
    "    return class_counts\n",
    "\n",
    "stats = analyze_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4385d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†:\n",
      "  Train: 56Í∞ú\n",
      "  Val: 14Í∞ú\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:00<00:00, 1961.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train - Images: 56, Annotations: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [00:00<00:00, 2865.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val - Images: 14, Annotations: 14\n",
      "\n",
      "‚úÖ COCO ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å:\n",
      "  Train: /Users/handaeseong/dev/data-engineer/workspaces/z-mini_project/fruits/efficientdet/outputs/train_cate1.json\n",
      "  Val: /Users/handaeseong/dev/data-engineer/workspaces/z-mini_project/fruits/efficientdet/outputs/val_cate1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. COCO ÌòïÏãù Îç∞Ïù¥ÌÑ∞ÏÖã Î≥ÄÌôò\n",
    "class FruitDatasetConverter:\n",
    "    \"\"\"JSON to COCO format converter for cate1 (fruit type)\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir, labels_dir, output_dir):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # cate1 Í∏∞Î∞ò ÌÅ¥ÎûòÏä§ Îß§Ìïë\n",
    "        self.class_map = {\n",
    "            'ÏÇ¨Í≥º': 0,\n",
    "            'Î∞∞': 1,\n",
    "            'Í∞ê': 2\n",
    "        }\n",
    "        self.classes = ['ÏÇ¨Í≥º', 'Î∞∞', 'Í∞ê']\n",
    "        \n",
    "    def convert_to_coco(self, split_ratio=0.8):\n",
    "        \"\"\"COCO ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò Î∞è train/val Î∂ÑÌï†\"\"\"\n",
    "        \n",
    "        json_files = sorted(glob.glob(os.path.join(self.labels_dir, '*.json')))\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(json_files)\n",
    "        \n",
    "        split_idx = int(len(json_files) * split_ratio)\n",
    "        train_files = json_files[:split_idx]\n",
    "        val_files = json_files[split_idx:]\n",
    "        \n",
    "        print(f\"\\nÎç∞Ïù¥ÌÑ∞ Î∂ÑÌï†:\")\n",
    "        print(f\"  Train: {len(train_files)}Í∞ú\")\n",
    "        print(f\"  Val: {len(val_files)}Í∞ú\")\n",
    "        \n",
    "        # Train/Val Í∞ÅÍ∞Å Î≥ÄÌôò\n",
    "        train_coco = self._create_coco_dataset(train_files, 'train')\n",
    "        val_coco = self._create_coco_dataset(val_files, 'val')\n",
    "        \n",
    "        # Ï†ÄÏû•\n",
    "        train_json_path = os.path.join(self.output_dir, 'train_cate1.json')\n",
    "        val_json_path = os.path.join(self.output_dir, 'val_cate1.json')\n",
    "        \n",
    "        with open(train_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(train_coco, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(val_json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(val_coco, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"\\n‚úÖ COCO ÌååÏùº ÏÉùÏÑ± ÏôÑÎ£å:\")\n",
    "        print(f\"  Train: {train_json_path}\")\n",
    "        print(f\"  Val: {val_json_path}\")\n",
    "        \n",
    "        return train_json_path, val_json_path\n",
    "    \n",
    "    def _create_coco_dataset(self, json_files, split_name):\n",
    "        \"\"\"COCO Îç∞Ïù¥ÌÑ∞ÏÖã Íµ¨Ï°∞ ÏÉùÏÑ±\"\"\"\n",
    "        \n",
    "        coco_dataset = {\n",
    "            \"images\": [],\n",
    "            \"annotations\": [],\n",
    "            \"categories\": []\n",
    "        }\n",
    "        \n",
    "        # Categories Ï∂îÍ∞Ä\n",
    "        for idx, class_name in enumerate(self.classes):\n",
    "            coco_dataset[\"categories\"].append({\n",
    "                \"id\": idx,\n",
    "                \"name\": class_name,\n",
    "                \"supercategory\": \"fruit\"\n",
    "            })\n",
    "        \n",
    "        annotation_id = 0\n",
    "        \n",
    "        for image_id, json_file in enumerate(tqdm(json_files, desc=f\"Converting {split_name}\")):\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Ïù¥ÎØ∏ÏßÄ ÌååÏùºÎ™Ö\n",
    "            img_filename = os.path.basename(json_file).replace('.json', '.png')\n",
    "            img_path = os.path.join(self.images_dir, img_filename)\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                continue\n",
    "            \n",
    "            # Ïù¥ÎØ∏ÏßÄ Ï†ïÎ≥¥\n",
    "            img = Image.open(img_path)\n",
    "            width, height = img.size\n",
    "            \n",
    "            coco_dataset[\"images\"].append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": img_filename,\n",
    "                \"width\": width,\n",
    "                \"height\": height\n",
    "            })\n",
    "            \n",
    "            # Annotation Ï∂îÍ∞Ä\n",
    "            cate1 = data.get('cate1', '')\n",
    "            if cate1 not in self.class_map:\n",
    "                continue\n",
    "            \n",
    "            bbox = data.get('bndbox', {})\n",
    "            xmin = float(bbox.get('xmin', 0))\n",
    "            ymin = float(bbox.get('ymin', 0))\n",
    "            xmax = float(bbox.get('xmax', width))\n",
    "            ymax = float(bbox.get('ymax', height))\n",
    "            \n",
    "            bbox_width = xmax - xmin\n",
    "            bbox_height = ymax - ymin\n",
    "            area = bbox_width * bbox_height\n",
    "            \n",
    "            coco_dataset[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": self.class_map[cate1],\n",
    "                \"bbox\": [xmin, ymin, bbox_width, bbox_height],  # COCO format: [x, y, w, h]\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            \n",
    "            annotation_id += 1\n",
    "        \n",
    "        print(f\"{split_name} - Images: {len(coco_dataset['images'])}, Annotations: {len(coco_dataset['annotations'])}\")\n",
    "        \n",
    "        return coco_dataset\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î≥ÄÌôò Ïã§Ìñâ\n",
    "converter = FruitDatasetConverter(IMAGES_DIR, LABELS_DIR, OUTPUT_DIR)\n",
    "train_json, val_json = converter.convert_to_coco(split_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "470cce65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú ÏôÑÎ£å:\n",
      "  Train ÏÉòÌîå Ïàò: 56\n",
      "  Val ÏÉòÌîå Ïàò: 14\n",
      "\n",
      "ÏÉòÌîå Îç∞Ïù¥ÌÑ∞:\n",
      "  Image shape: torch.Size([3, 512, 512])\n",
      "  Boxes: [[   0.    0. 1000. 1000.]]\n",
      "  Labels: [0]\n"
     ]
    }
   ],
   "source": [
    "# 4. PyTorch Dataset ÌÅ¥ÎûòÏä§\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class COCOFruitDataset(Dataset):\n",
    "    \"\"\"COCO ÌòïÏãù Í≥ºÏùº Îç∞Ïù¥ÌÑ∞ÏÖã\"\"\"\n",
    "    \n",
    "    def __init__(self, images_dir, coco_json_path, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(coco_json_path, 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        self.images = self.coco_data['images']\n",
    "        self.annotations = self.coco_data['annotations']\n",
    "        \n",
    "        # image_idÎ•º ÌÇ§Î°ú ÌïòÎäî annotations ÎîïÏÖîÎÑàÎ¶¨ ÏÉùÏÑ±\n",
    "        self.img_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_to_anns:\n",
    "                self.img_to_anns[img_id] = []\n",
    "            self.img_to_anns[img_id].append(ann)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_info = self.images[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_info['file_name'])\n",
    "        \n",
    "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        # Annotations Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        img_id = img_info['id']\n",
    "        anns = self.img_to_anns.get(img_id, [])\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        for ann in anns:\n",
    "            x, y, w, h = ann['bbox']\n",
    "            boxes.append([x, y, x + w, y + h])  # [xmin, ymin, xmax, ymax]\n",
    "            labels.append(ann['category_id'])\n",
    "        \n",
    "        boxes = np.array(boxes, dtype=np.float32)\n",
    "        labels = np.array(labels, dtype=np.int64)\n",
    "        \n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': img_id\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, target\n",
    "\n",
    "# Transform Ï†ïÏùò\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è DataLoader ÏÉùÏÑ±\n",
    "train_dataset = COCOFruitDataset(IMAGES_DIR, train_json, transform=transform)\n",
    "val_dataset = COCOFruitDataset(IMAGES_DIR, val_json, transform=transform)\n",
    "\n",
    "print(f\"\\n‚úÖ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú ÏôÑÎ£å:\")\n",
    "print(f\"  Train ÏÉòÌîå Ïàò: {len(train_dataset)}\")\n",
    "print(f\"  Val ÏÉòÌîå Ïàò: {len(val_dataset)}\")\n",
    "\n",
    "# ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÌôïÏù∏\n",
    "sample_img, sample_target = train_dataset[0]\n",
    "print(f\"\\nÏÉòÌîå Îç∞Ïù¥ÌÑ∞:\")\n",
    "print(f\"  Image shape: {sample_img.shape}\")\n",
    "print(f\"  Boxes: {sample_target['boxes']}\")\n",
    "print(f\"  Labels: {sample_target['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e88341b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:timm.models._builder:Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ EfficientDet-D1 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
      "  Device: cpu\n",
      "  Input size: [512, 512]\n",
      "  Classes: 3\n"
     ]
    }
   ],
   "source": [
    "# 5. EfficientDet Î™®Îç∏ Î°úÎìú (effdet ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö©)\n",
    "# effdet ÏÑ§Ïπò: !pip install effdet\n",
    "\n",
    "try:\n",
    "    from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "    from effdet.efficientdet import HeadNet\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  effdet ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏÑ§ÏπòÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.\")\n",
    "    print(\"Îã§Ïùå Î™ÖÎ†πÏñ¥Î°ú ÏÑ§ÏπòÌïòÏÑ∏Ïöî:\")\n",
    "    print(\"!pip install effdet\")\n",
    "    print(\"ÎòêÎäî conda ÌôòÍ≤ΩÏóêÏÑú:\")\n",
    "    print(\"pip install effdet\")\n",
    "\n",
    "# EfficientDet-D1 ÏÑ§Ï†ï\n",
    "def create_model(num_classes=3):\n",
    "    \"\"\"EfficientDet-D1 Î™®Îç∏ ÏÉùÏÑ±\"\"\"\n",
    "    \n",
    "    config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "    config.num_classes = num_classes\n",
    "    config.image_size = (512, 512)\n",
    "    \n",
    "    net = EfficientDet(config, pretrained_backbone=True)\n",
    "    net.class_net = HeadNet(\n",
    "        config,\n",
    "        num_outputs=config.num_classes,\n",
    "    )\n",
    "    \n",
    "    model = DetBenchTrain(net, config)\n",
    "    \n",
    "    return model, config\n",
    "\n",
    "model, config = create_model(num_classes=3)\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\n‚úÖ EfficientDet-D1 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Input size: {config.image_size}\")\n",
    "print(f\"  Classes: {config.num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0db91a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ DataLoader ÏÉùÏÑ± ÏôÑÎ£å\n",
      "  Train batches: 14\n",
      "  Val batches: 4\n"
     ]
    }
   ],
   "source": [
    "# 6. ÌïôÏäµ Ìï®Ïàò Ï†ïÏùò\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Î∞∞Ïπò Îç∞Ïù¥ÌÑ∞ collate Ìï®Ïàò\"\"\"\n",
    "    images = []\n",
    "    targets = []\n",
    "    \n",
    "    for img, target in batch:\n",
    "        images.append(img)\n",
    "        targets.append(target)\n",
    "    \n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    return images, targets\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device, epoch):\n",
    "    \"\"\"1 ÏóêÌè≠ ÌïôÏäµ\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    num_batches = 0  # Ïù¥ Ï§Ñ Ï∂îÍ∞Ä\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for images, targets in progress_bar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # EfficientDetÏùÄ targetsÎ•º ÌäπÎ≥ÑÌïú ÌòïÏãùÏúºÎ°ú Î∞õÏùå\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # targetsÎ•º Î™®Îç∏Ïù¥ ÏöîÍµ¨ÌïòÎäî ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "        target_dict = {\n",
    "            'bbox': [],\n",
    "            'cls': []\n",
    "        }\n",
    "        \n",
    "        for target in targets:\n",
    "            boxes = torch.tensor(target['boxes'], dtype=torch.float32)\n",
    "            labels = torch.tensor(target['labels'], dtype=torch.int64)\n",
    "            \n",
    "            target_dict['bbox'].append(boxes)\n",
    "            target_dict['cls'].append(labels)\n",
    "        \n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            output = model(images, target_dict)\n",
    "            \n",
    "            # outputÏù¥ ÌäúÌîåÏù∏ Í≤ΩÏö∞ Ï≤´ Î≤àÏß∏ ÏöîÏÜåÍ∞Ä loss\n",
    "            if isinstance(output, tuple):\n",
    "                loss = output[0]\n",
    "            else:\n",
    "                loss = output\n",
    "            \n",
    "            # lossÍ∞Ä dictÏù∏ Í≤ΩÏö∞\n",
    "            if isinstance(loss, dict):\n",
    "                loss = sum(loss.values())\n",
    "            \n",
    "            # lossÍ∞Ä ÌÖêÏÑúÏù∏ÏßÄ ÌôïÏù∏\n",
    "            if isinstance(loss, torch.Tensor):\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "            else:\n",
    "                print(f\"\\n‚ö†Ô∏è  ÏòàÏÉÅÏπò Î™ªÌïú loss ÌÉÄÏûÖ: {type(loss)}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  Î∞∞Ïπò Ï≤òÎ¶¨ Ï§ë ÏóêÎü¨ Î∞úÏÉù: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    return avg_loss\n",
    "\n",
    "# DataLoader ÏÉùÏÑ±\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,  # CPUÏóêÏÑúÎäî ÏûëÏùÄ Î∞∞Ïπò ÏÇ¨Ïö©\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ DataLoader ÏÉùÏÑ± ÏôÑÎ£å\")\n",
    "print(f\"  Train batches: {len(train_loader)}\")\n",
    "print(f\"  Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256e9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ ÌïôÏäµ ÏãúÏûë...\n",
      "  Epochs: 50\n",
      "  Learning Rate: 0.0001\n",
      "  Optimizer: AdamW\n",
      "  Scheduler: CosineAnnealingLR\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:04<00:00,  4.62s/it, loss=7.6e+3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 7706.9957 - LR: 0.000100\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 7706.9957)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:03<00:00,  4.56s/it, loss=7.39e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 - Loss: 7484.1602 - LR: 0.000100\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 7484.1602)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:05<00:00,  4.66s/it, loss=7.2e+3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 - Loss: 7288.5182 - LR: 0.000099\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 7288.5182)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:04<00:00,  4.63s/it, loss=7e+3]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 - Loss: 7096.9813 - LR: 0.000098\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 7096.9813)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:06<00:00,  4.73s/it, loss=6.81e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 - Loss: 6900.5013 - LR: 0.000098\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 6900.5013)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:08<00:00,  4.86s/it, loss=6.61e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 - Loss: 6699.0772 - LR: 0.000097\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 6699.0772)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:10<00:00,  5.05s/it, loss=6.39e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 - Loss: 6491.4067 - LR: 0.000095\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 6491.4067)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:15<00:00,  5.39s/it, loss=6.18e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 - Loss: 6277.2597 - LR: 0.000094\n",
      "  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: 6277.2597)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:39<00:28,  4.82s/it, loss=6.06e+3]"
     ]
    }
   ],
   "source": [
    "# 7. ÌïôÏäµ Ïã§Ìñâ\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
    "\n",
    "print(f\"\\nüöÄ ÌïôÏäµ ÏãúÏûë...\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Scheduler: CosineAnnealingLR\")\n",
    "print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "train_losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ÌïôÏäµ\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Learning rate ÏóÖÎç∞Ïù¥Ìä∏\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {train_loss:.4f} - LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Î™®Îç∏ Ï†ÄÏû• (Best)\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "        }, os.path.join(OUTPUT_DIR, 'best_efficientdet_cate1.pth'))\n",
    "        print(f\"  ‚úÖ Best Î™®Îç∏ Ï†ÄÏû• (Loss: {best_loss:.4f})\")\n",
    "    \n",
    "    # Ï£ºÍ∏∞Ï†Å Ï†ÄÏû• (10 epochÎßàÎã§)\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_loss,\n",
    "        }, os.path.join(OUTPUT_DIR, f'efficientdet_cate1_epoch{epoch+1}.pth'))\n",
    "        print(f\"  üíæ Checkpoint Ï†ÄÏû• (Epoch {epoch+1})\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\nüéâ ÌïôÏäµ ÏôÑÎ£å!\")\n",
    "print(f\"  Best Loss: {best_loss:.4f}\")\n",
    "print(f\"  Î™®Îç∏ Ï†ÄÏû• Í≤ΩÎ°ú: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed94fc3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 8. ÌïôÏäµ Í≤∞Í≥º ÏãúÍ∞ÅÌôî\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m, fontsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# 8. ÌïôÏäµ Í≤∞Í≥º ÏãúÍ∞ÅÌôî\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('EfficientDet-D1 Training Loss (cate1)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plot_path = os.path.join(OUTPUT_DIR, 'training_loss_cate1.png')\n",
    "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ ÌïôÏäµ Í∑∏ÎûòÌîÑ Ï†ÄÏû•: {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a61c5fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DetBenchTrain.forward() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úÖ ÏòàÏ∏° ÏãúÍ∞ÅÌôî Ï†ÄÏû•: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mviz_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# ÏãúÍ∞ÅÌôî Ïã§Ìñâ\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[43mvisualize_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mvisualize_predictions\u001b[0;34m(model, dataset, device, num_samples)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Ï∂îÎ°†\u001b[39;00m\n\u001b[1;32m     17\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Ïù¥ÎØ∏ÏßÄ denormalize\u001b[39;00m\n\u001b[1;32m     21\u001b[0m img_np \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/dev/data-engineer/miniconda3/envs/effdet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/data-engineer/miniconda3/envs/effdet_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: DetBenchTrain.forward() missing 1 required positional argument: 'target'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9. Ï∂îÎ°† Î∞è ÏãúÍ∞ÅÌôî\n",
    "def visualize_predictions(model, dataset, device, num_samples=5):\n",
    "    \"\"\"ÏòàÏ∏° Í≤∞Í≥º ÏãúÍ∞ÅÌôî\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    class_names = ['ÏÇ¨Í≥º', 'Î∞∞', 'Í∞ê']\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(min(num_samples, len(dataset))):\n",
    "            img, target = dataset[i]\n",
    "            \n",
    "            # Ï∂îÎ°†\n",
    "            img_tensor = img.unsqueeze(0).to(device)\n",
    "            output = model(img_tensor)\n",
    "            \n",
    "            # Ïù¥ÎØ∏ÏßÄ denormalize\n",
    "            img_np = img.cpu().numpy().transpose(1, 2, 0)\n",
    "            img_np = img_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            img_np = np.clip(img_np, 0, 1)\n",
    "            \n",
    "            axes[i].imshow(img_np)\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f'Sample {i+1}', fontsize=10, fontweight='bold')\n",
    "            \n",
    "            # Ground Truth Î∞ïÏä§ Í∑∏Î¶¨Í∏∞\n",
    "            gt_boxes = target['boxes']\n",
    "            gt_labels = target['labels']\n",
    "            \n",
    "            for box, label in zip(gt_boxes, gt_labels):\n",
    "                x1, y1, x2, y2 = box\n",
    "                # Ï†ïÍ∑úÌôîÎêú Ï¢åÌëúÎ•º Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞Î°ú Î≥ÄÌôò\n",
    "                width, height = 512, 512\n",
    "                x1, x2 = x1 * width / 512, x2 * width / 512\n",
    "                y1, y2 = y1 * height / 512, y2 * height / 512\n",
    "                \n",
    "                rect = plt.Rectangle(\n",
    "                    (x1, y1), x2 - x1, y2 - y1,\n",
    "                    fill=False, color=colors[label], linewidth=2\n",
    "                )\n",
    "                axes[i].add_patch(rect)\n",
    "                axes[i].text(\n",
    "                    x1, y1 - 5,\n",
    "                    f'{class_names[label]}',\n",
    "                    color=colors[label],\n",
    "                    fontsize=9,\n",
    "                    fontweight='bold',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.7)\n",
    "                )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    viz_path = os.path.join(OUTPUT_DIR, 'predictions_cate1.png')\n",
    "    plt.savefig(viz_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úÖ ÏòàÏ∏° ÏãúÍ∞ÅÌôî Ï†ÄÏû•: {viz_path}\")\n",
    "\n",
    "# ÏãúÍ∞ÅÌôî Ïã§Ìñâ\n",
    "visualize_predictions(model, val_dataset, device, num_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf5cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effdet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
