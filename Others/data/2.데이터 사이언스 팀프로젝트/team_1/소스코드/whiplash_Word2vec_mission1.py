# =========================================
# ğŸ“° í”„ë¡œì íŠ¸ëª… : ë‰´ìŠ¤ ë°ì´í„° í•™ìŠµì„ í†µí•œ ê°€ì§œ ë‰´ìŠ¤ ë¶„ë¥˜
# ê°œì„  ëª©í‘œ : AUROC â‰¥ 0.5 + Cosine Similarity ë¶„ì„ ì¶”ê°€
# =========================================
# âœ… ì£¼ìš” ê¸°ëŠ¥ ìš”ì•½
# 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë°ì´í„° ë¡œë“œ
# 2. Word2Vec í•™ìŠµ ë° ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
# 3. BiLSTM + CNN í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ êµ¬ì„±
# 4. AUROC, Precision, Pearson r ê³„ì‚°
# 5. ì œëª©-ë³¸ë¬¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„ì„ + ì‹œê°í™”
# =========================================

import os, re
import chardet
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from gensim.models import Word2Vec
from sklearn.metrics import (
    classification_report, roc_auc_score, precision_score,
    accuracy_score, confusion_matrix
)
from scipy.stats import pearsonr
from scipy.spatial.distance import cosine
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Embedding, Conv1D, GlobalMaxPooling1D, concatenate,
    Dense, Dropout, Input, BatchNormalization, LSTM, Bidirectional
)
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight

# -----------------------
# Config
# -----------------------
TRAIN_CSV = r"c:\workspaces\fake\data\mission1_train.csv"
TEST_CSV = r"c:\workspaces\fake\data\mission1_test.csv"
EMBED_DIM = 128
MAXLEN = 500
NUM_WORDS = 30000
BATCH_SIZE = 512
EPOCHS = 100
FILTER_SIZE = 3
NUM_FILTERS = 1024
DROPOUT_RATE = 0.4
L2_ALPHA = 0.001

# -----------------------
# í…ìŠ¤íŠ¸ ì •ì œ
# -----------------------
def clean_text(text):
    if not isinstance(text, str): return ""
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    stopwords = ["ê¸°ì","ì‚¬ì§„","ì—°í•©ë‰´ìŠ¤","ë‰´ìŠ¤","ë³´ë„","ì˜¤ëŠ˜","ëŒ€í•œ","ì—ì„œ"]
    for sw in stopwords:
        text = text.replace(sw, "")
    return text

# -----------------------
# ë°ì´í„° ë¡œë“œ
# -----------------------
def load_split_data(train_path=TRAIN_CSV, test_path=TEST_CSV):
    def safe_read_csv(path):
        with open(path, 'rb') as f:
            enc = chardet.detect(f.read(200000))['encoding']
        for enc_try in [enc, 'utf-8-sig', 'euc-kr', 'cp949']:
            try:
                return pd.read_csv(path, encoding=enc_try)
            except: continue
        return pd.read_csv(path, encoding='utf-8', errors='replace')

    train_df = safe_read_csv(train_path)
    test_df = safe_read_csv(test_path)
    train_df.columns = [c.lower().strip() for c in train_df.columns]
    test_df.columns = [c.lower().strip() for c in test_df.columns]
    rename_map = {'ë‰´ìŠ¤ì œëª©':'title','ë³¸ë¬¸':'content','ê°€ì§œë‰´ìŠ¤ì—¬ë¶€':'label'}
    train_df.rename(columns=rename_map, inplace=True)
    test_df.rename(columns=rename_map, inplace=True)

    for df in [train_df, test_df]:
        df["title"] = df["title"].fillna("").apply(clean_text)
        df["content"] = df["content"].fillna("").apply(clean_text)
        df["text"] = (df["title"] + " " + df["content"]).str.strip()
        df["label"] = df["label"].fillna(0).astype(int)

    print("âœ… CSV ë¡œë“œ ì™„ë£Œ:", train_df.shape, test_df.shape)
    return train_df, test_df

# -----------------------
# Word2Vec í•™ìŠµ
# -----------------------
def train_word2vec(sentences, vector_size=EMBED_DIM, window=8, min_count=1):
    tokenized = [s.split() for s in sentences]
    model = Word2Vec(sentences=tokenized, vector_size=vector_size,
                     window=window, min_count=min_count, workers=4, sg=1)
    return model

# -----------------------
# ì„ë² ë”© ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„±
# -----------------------
def build_embedding_matrix(tokenizer, w2v_model, vector_size=EMBED_DIM):
    vocab_size = len(tokenizer.word_index) + 1
    emb = np.random.normal(0, 0.1, (vocab_size, vector_size))
    for w, i in tokenizer.word_index.items():
        if w in w2v_model.wv:
            emb[i] = w2v_model.wv[w]
    return emb

# -----------------------
# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
# -----------------------
def cosine_similarity_from_word2vec(df, w2v_model):
    def sentence_vector(sentence):
        words = [w for w in sentence.split() if w in w2v_model.wv]
        if not words: return np.zeros(w2v_model.vector_size)
        return np.mean([w2v_model.wv[w] for w in words], axis=0)
    sims = []
    for _, row in df.iterrows():
        v1, v2 = sentence_vector(row["title"]), sentence_vector(row["content"])
        sims.append(0 if np.all(v1==0) or np.all(v2==0) else 1 - cosine(v1, v2))
    df["cosine_similarity"] = sims
    return df

# -----------------------
# í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ì˜ (BiLSTM + CNN)
# -----------------------
def build_hybrid_model(vocab_size, emb_matrix):
    inp = Input(shape=(MAXLEN,))
    x = Embedding(input_dim=vocab_size, output_dim=emb_matrix.shape[1],
                  weights=[emb_matrix], input_length=MAXLEN, trainable=True)(inp)
    
    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(x)
    conv3 = Conv1D(64, 3, activation='relu', padding='same')(lstm_out)
    conv4 = Conv1D(64, 4, activation='relu', padding='same')(lstm_out)
    conv5 = Conv1D(64, 5, activation='relu', padding='same')(lstm_out)
    
    merged = concatenate([GlobalMaxPooling1D()(conv3),
                          GlobalMaxPooling1D()(conv4),
                          GlobalMaxPooling1D()(conv5)])
    merged = BatchNormalization()(merged)
    merged = Dropout(0.4)(merged)
    merged = Dense(128, activation='relu')(merged)
    merged = Dropout(0.3)(merged)
    out = Dense(2, activation='softmax')(merged)
    
    model = Model(inputs=inp, outputs=out)
    model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# -----------------------
# í•™ìŠµ ê³¡ì„  ì‹œê°í™”
# -----------------------
def plot_training_history(history):
    plt.figure(figsize=(12,4))
    plt.subplot(1,2,1)
    plt.plot(history.history["accuracy"], label="train_acc")
    plt.plot(history.history["val_accuracy"], label="val_acc")
    plt.legend(); plt.title("Accuracy Curve")

    plt.subplot(1,2,2)
    plt.plot(history.history["loss"], label="train_loss")
    plt.plot(history.history["val_loss"], label="val_loss")
    plt.legend(); plt.title("Loss Curve")
    plt.show()

# -----------------------
# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„ì„ ì‹œê°í™”
# -----------------------
def visualize_cosine_similarity(df):
    plt.figure(figsize=(8,5))
    sns.histplot(df["cosine_similarity"], bins=30, kde=True, color='skyblue')
    plt.title("Cosine Similarity Distribution (Title vs Content)")
    plt.xlabel("Cosine Similarity")
    plt.ylabel("Frequency")
    plt.show()

    print("\nğŸ“Š ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í†µê³„:")
    print(df["cosine_similarity"].describe())

    top5 = df.sort_values("cosine_similarity", ascending=False).head(5)[["title", "cosine_similarity"]]
    low5 = df.sort_values("cosine_similarity", ascending=True).head(5)[["title", "cosine_similarity"]]
    print("\nğŸŸ¢ ìœ ì‚¬ë„ ìƒìœ„ ê¸°ì‚¬ 5ê°œ:")
    print(top5.to_string(index=False))
    print("\nğŸ”´ ìœ ì‚¬ë„ í•˜ìœ„ ê¸°ì‚¬ 5ê°œ:")
    print(low5.to_string(index=False))

# -----------------------
# ë©”ì¸ ë¡œì§
# -----------------------
def main():
    # 1ï¸âƒ£ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    train_df, test_df = load_split_data()
    X_train, y_train = train_df["text"], train_df["label"].values
    X_test, y_test = test_df["text"], test_df["label"].values

    # 2ï¸âƒ£ í† í¬ë‚˜ì´ì € ë° ì‹œí€€ìŠ¤ ë³€í™˜
    tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train)
    X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=MAXLEN, padding='post')
    X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=MAXLEN, padding='post')

    # 3ï¸âƒ£ Word2Vec í•™ìŠµ ë° ì„ë² ë”© êµ¬ì„±
    print("Training Word2Vec...")
    w2v = train_word2vec(X_train.tolist())
    emb_matrix = build_embedding_matrix(tokenizer, w2v)
    vocab_size = emb_matrix.shape[0]

    # 4ï¸âƒ£ ì œëª©-ë³¸ë¬¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    test_df = cosine_similarity_from_word2vec(test_df, w2v)

    # 5ï¸âƒ£ ëª¨ë¸ êµ¬ì„± ë° í•™ìŠµ
    model = build_hybrid_model(vocab_size, emb_matrix)
    model.summary()

    y_train_cat = to_categorical(y_train, 2)
    y_test_cat = to_categorical(y_test, 2)
    class_weights = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))

    es = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)
    rl = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)

    history = model.fit(
        X_train_seq, y_train_cat, validation_split=0.2,
        epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[es, rl],
        class_weight=class_weights, verbose=1
    )

    # 6ï¸âƒ£ ì˜ˆì¸¡ ë° ì„±ëŠ¥ í‰ê°€
    y_prob = model.predict(X_test_seq)[:,1]
    y_pred = (y_prob >= 0.5).astype(int)

    print("\nğŸ¯ í‰ê°€ ê²°ê³¼")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("AUROC:", roc_auc_score(y_test, y_prob))
    print("Pearson r:", pearsonr(y_test, y_prob)[0])
    print(classification_report(y_test, y_pred, digits=4))

    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
    plt.title("Confusion Matrix")
    plt.show()

    plot_training_history(history)

    # 7ï¸âƒ£ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë¶„ì„ ì‹œê°í™”
    visualize_cosine_similarity(test_df)

# -----------------------
if __name__ == "__main__":
    main()
