==========================================================================================
1. ml26 for문에서 x,y를 만드는 원리에 대해 공부해보기
==========================================================================================
x = []						# x라는 리스트를 만듭니다.
y = []						# y라는 리스트를 만듭니다.

for idx, row in mr.iterrows(): 		# 모든 데이터 행을 하나씩 반복합니다.
    y.append(row.iloc[0])			# row의 0번째인 type의 첫번째 열을 y애 추가합니다.

    row_data =[]				# row_data라는 리스트를 만듭니다.				
    for data in row.iloc[1:]:		# row의 1번째부터 마지막까지 열까지 반복합니다.
        row_data.append(ord(data))	# ord() 함수를 통해 data 의 문자열을 아스키코드로 변경하고 row_data에 추가합니다.

    x.append(row_data)			# x에 row_data를 추가합니다.

print(x[:5])					# x의 항목 5개만 출력
print(y[:5])					# y의 항목 5개만 출력

==========================================================================================
2. ml29 xgboost 설명과 특징 공부해보기 (https://xgboost.readthedocs.io/en/stable/**)**
==========================================================================================
XGBoost(Extreme Gradient Boosting)는 그래디언트 부스팅(Gradient Boosting) 알고리즘을 기반으로 구현된, 확장 가능하고 최적화된 앙상블(Ensemble) 머신러닝 라이브러리입니다.
여러 개의 약한 예측 모델(주로 결정 트리)을 순차적으로 학습시켜 이전 모델의 오류를 보완해나가는 '부스팅' 방식을 사용하되, 이를 극도로(Extreme) 효율적이고 빠르게 수행하도록 설계되었습니다.
XGBoost는 분류(Classification)와 회귀(Regression) 문제 모두에서 매우 뛰어난 성능을 보여주며, 캐글(Kaggle)과 같은 데이터 과학 경진대회에서 압도적으로 많이 사용되어 그 성능이 입증되었습니다.
==========================================================================================
3. eval_metric : 손실함수 (클래스 분류 - error / 다른 것들은?)
==========================================================================================
logloss: 음의 로그 가능도(Negative log-likelihood). 이진 분류에 사용됩니다.

error: 이진 분류 오류율. 예측값이 0.5보다 크면 1(positive), 아니면 0(negative)으로 간주하여 오류를 계산합니다. (#(틀린 예측) / #(전체))

error@t: 이진 분류 오류율 (임계값 t 지정). t에 지정한 값(예: 0.6)을 임계값으로 사용하여 오류를 계산합니다.

merror: 다중 분류(Multiclass) 오류율. (#(틀린 예측) / #(전체))

mlogloss: 다중 분류 로그 손실(Multiclass logloss).

auc: ROC 곡선 하단 면적(Area under the ROC Curve). 모델의 성능을 평가하는 대표적인 지표 중 하나입니다.

aucpr: 정밀도-재현율(Precision-Recall) 곡선 하단 면적. 특히 불균형 데이터셋(imbalanced dataset)의 분류 성능을 평가할 때 유용합니다.
==========================================================================================
4. lightGBM 특징에서 leaf wise 찾아보기
==========================================================================================
성장 방식: 대부분의 다른 트리 기반 알고리즘(예: XGBoost의 기본 방식)이 트리를 수평적으로, 즉 **level-wise (레벨 중심)**로 확장하는 것과 다릅니다. LightGBM은 leaf-wise (리프 중심), 즉 'best-first' 방식으로 트리를 수직적으로 확장합니다.

선택 기준: 트리를 분할할 때 현재 레벨의 모든 노드를 분할하는 것이 아니라, 전체 리프(leaf) 중에서 손실(loss)을 가장 크게 줄일 수 있는 리프(max delta loss를 가진 리프)를 하나 선택하여 분할합니다.

장점: level-wise 방식에 비해 더 빠르고 효율적으로 낮은 손실 값(높은 정확도)에 도달하는 경향이 있습니다.

단점 (주의사항): 데이터의 양이 적을 경우, 트리가 깊어지기만 하면서 과적합(over-fitting)이 발생하기 쉽습니다. 이를 방지하기 위해 max_depth 파라미터를 사용하여 트리의 최대 깊이를 명시적으로 제한하는 것이 권장됩니다.